# -*- coding: utf-8 -*-
"""ModeloCompetencia2_DMEF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S2x3191OFUYmAtkGLR5I_kxAuMg9FNkc
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install optuna==3.6.1

pip install --upgrade lightgbm

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import KFold
from sklearn.impute import SimpleImputer

import lightgbm as lgb


import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour

from time import time

import pickle

from google.colab import drive
drive.mount("/content/drive")

dataset_path = '/content/drive/MyDrive/MaestDataMining/Especializacion/2C/DMEF/Datasets/'
dataset_file = 'competencia_02_Ternaria_PaOpt.csv'

data = pd.read_csv(dataset_path + dataset_file)

# Eliminar la columna 'X'
data = data.drop(columns=['X'], errors='ignore')

data['foto_mes'] = data['foto_mes'].astype(str)  # Asegura que 'foto_mes' sea tipo string

# Filtrar y contar para cada mes por separado
meses_filtrados = ["202108", "202107", "202106", "202105"]

for mes in meses_filtrados:
    # Filtrar los datos para el mes específico
    data_mes = data[data['foto_mes'] == mes]

    # Comprobar si hay datos para el mes
    if not data_mes.empty:
        # Contar la cantidad de casos por categoría en 'clase_ternaria'
        conteo_clases = data_mes['clase_ternaria'].value_counts()

        # Mostrar el resultado para el mes
        print(f"Conteo de clases para {mes}:")
        print(conteo_clases)
    else:
        print(f"No hay datos para el mes {mes}")
    print("\n")


    # Junio es el último mes con datos para entrenar
    # Queremos predecir agosto

#Se está rompiendo más adelante, voy a borrar más meses. Dejo desde agsto 2020 (ahora dejo julio pero solo para calcular el lag y dlag de agosto 2020)

# Filtrar los datos para mantener solo los meses a partir de agosto 2020
data = data[(data['foto_mes'] >= "202007")]

#Quito columnas con 1er cuartil y mediana 0, porque se rompe la ram si meto todo
omit_columns = [
    "cliente_vip", "internet", "mcuenta_corriente_adicional", "mcaja_ahorro_adicional",
    "ctarjeta_master_transacciones", "mtarjeta_master_consumo", "cprestamos_personales", "mprestamos_personales",
    "cprestamos_prendarios", "mprestamos_prendarios", "cprestamos_hipotecarios", "mprestamos_hipotecarios",
    "cplazo_fijo", "mplazo_fijo_dolares", "mplazo_fijo_pesos", "cinversion1", "minversion1_pesos", "minversion1_dolares",
    "cinversion2", "minversion2", "cseguro_vida", "cseguro_auto", "cseguro_vivienda", "cseguro_accidentes_personales",
    "ccaja_seguridad", "mpayroll2", "cpayroll2_trx", "ccuenta_debitos_automaticos", "mcuenta_debitos_automaticos",
    "ctarjeta_master_debitos_automaticos", "mttarjeta_master_debitos_automaticos", "cpagodeservicios", "mpagodeservicios",
    "cpagomiscuentas", "mpagomiscuentas", "ccajeros_propios_descuentos", "mcajeros_propios_descuentos",
    "ctarjeta_visa_descuentos", "mtarjeta_visa_descuentos", "ctarjeta_master_descuentos", "mtarjeta_master_descuentos",
    "ccomisiones_mantenimiento", "mcomisiones_mantenimiento", "cforex", "cforex_buy", "mforex_buy", "cforex_sell",
    "mforex_sell", "cextraccion_autoservicio", "mextraccion_autoservicio", "ccheques_depositados",
    "mcheques_depositados", "ccheques_emitidos", "mcheques_emitidos", "ccheques_depositados_rechazados",
    "mcheques_depositados_rechazados", "ccheques_emitidos_rechazados", "mcheques_emitidos_rechazados", "tcallcenter",
    "ccallcenter_transacciones", "ccajas_transacciones", "ccajas_consultas", "ccajas_depositos", "ccajas_extracciones",
    "ccajas_otras", "catm_trx", "matm", "catm_trx_other", "matm_other", "tmobile_app", "Master_delinquency",
    "Master_status", "Master_msaldototal", "Master_msaldopesos", "Master_msaldodolares", "Master_mconsumosdolares",
    "Master_madelantopesos", "Master_madelantodolares", "Master_mpagado", "Master_mpagosdolares",
    "Master_cadelantosefectivo", "Master_mpagominimo", "Visa_delinquency", "Visa_status", "Visa_Finiciomora",
    "Visa_msaldodolares", "Visa_mconsumosdolares", "Visa_madelantopesos", "Visa_madelantodolares", "Visa_mpagado",
    "Visa_mpagosdolares", "Visa_cadelantosefectivo"
]

# Eliminar columnas
data = data.drop(columns=omit_columns)

# Obtener y mostrar los nombres de las columnas del DataFrame
column_names = data.columns.tolist()

# Imprimir los nombres de las columnas
print("Nombres de las columnas del DataFrame:")
print(column_names)

# Sigo sacando columnas, porque si no no puedo correr
columns_to_keep = [
    'clase_ternaria', 'numero_de_cliente', 'foto_mes',
    'mpayroll', 'mcuenta_corriente',
    'mrentabilidad', 'mrentabilidad_annual', 'mcomisiones',
    'ctrx_quarter', 'mtransferencias_emitidas',
    'ctarjeta_visa_transacciones', 'mtarjeta_visa_consumo', 'cproductos'
]

# Seleccionar únicamente las columnas deseadas. Uso estas, más adelante uso más cuando tenga mejor ram
data = data[columns_to_keep]

# Calcular los percentiles, omitiendo las columnas esas
for col in data.columns:
    if col not in ['numero_de_cliente', 'foto_mes', 'clase_ternaria']:
        data[f'percentile_{col}'] = data[col].rank(pct=True)

print(data.shape)
data.head()

# Esto corre mal, rehacer
# Agrego Lag y Dlag para las variables

# Columnas que deben excluirse
# exclude_columns = ['numero_de_cliente', 'foto_mes', 'clase_ternaria']

# Iterar sobre las columnas que no están en la lista de exclusión
#for col in data.columns:
#    if col not in exclude_columns:
#        # Crear la columna 'lag' (valor del mes anterior)
#        data[f'{col}_lag'] = data.groupby('numero_de_cliente')[col].shift(1)

        # Crear la columna 'deltaLag' (diferencia entre original y lag)
#        data[f'{col}_deltaLag'] = data[col] - data[f'{col}_lag']

# Verificar el DataFrame resultante
#print(data.head())

# Guardar los datos correspondientes a agosto 2021 en un nuevo DataFrame para predicciones, que e slo que va finalmente a kaggle
data_agosto_2021 = data[data['foto_mes'] == "202108"]

# Ahora sí quito julios

# Quito el mes de prueba final, y los julios (el 2020 fue solo para calcular lags y delta lag, y el de 2021 no tiene info de clase ternaria)
data = data[~data['foto_mes'].isin(["202007", "202107", "202108"])]

# Ver los niveles únicos (valores distintos) de la variable 'foto_mes'
print(data['foto_mes'].unique())

# Ver el número de ocurrencias de cada mes en 'foto_mes'
print(data['foto_mes'].value_counts())

# Ver los niveles únicos (valores distintos) de la variable 'foto_mes'
print(data_agosto_2021['foto_mes'].unique())

# Ver el número de ocurrencias de cada mes en 'foto_mes'
print(data_agosto_2021['foto_mes'].value_counts())

# Parámetros de ganancia
ganancia_acierto = 273000
costo_estimulo = 7000

# Definir los meses de entrenamiento, validación y prueba. Estos son para cuando lo haga entero (saco agosto,que estoyjugado e tiempo)
mes_train = ["202010", "202011",
             "202012", "202101", "202102", "202103", "202104", "202105"]

mes_test = "202106"

"""Vamos a asignar pesos a las clases. En unos minutos explicaremos las razones detrás de esta decisión. Mientras tanto, pueden aprovechar el código para ajustar el peso de la clase **BAJA+2** según lo deseen.

clase_binaria2: toma el valor 1 si clase_ternaria es diferente de 'CONTINUA'
"""

# Asignación de pesos a las clases
data['clase_peso'] = 1.0
data.loc[data['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002
data.loc[data['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001

# Crear la clase binaria para el modelo
data['clase_binaria'] = np.where(data['clase_ternaria'] == 'BAJA+2', 1, 0)

# Dividir los datos en entrenamiento y prueba
train_data = data[data['foto_mes'].isin(mes_train)].drop(['foto_mes', 'numero_de_cliente'], axis=1)
test_data = data[data['foto_mes'] == mes_test].drop(['foto_mes', 'numero_de_cliente'], axis=1)

# Separar características (X), etiquetas (y) y pesos (w) para cada conjunto de datos
X_train = train_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria'], axis=1)
y_train = train_data['clase_binaria']
w_train = train_data['clase_peso']

X_test = test_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria'], axis=1)
y_test = np.where(test_data['clase_ternaria'] == 'BAJA+2', 1, 0)
w_test = test_data['clase_peso']

# Imputar los valores faltantes en los datos de prueba
imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')
Xif = imp_mean.fit_transform(X_test)

print(data.shape)
print(train_data.shape)
print(test_data.shape)
print(data_agosto_2021.shape)

"""Para evaluar la calidad del modelo, crearemos nuestra propia función de evaluación que calcule la ganancia. La razón de incluir los pesos es precisamente para poder implementar esta función de evaluación de manera adecuada. Al combinar las clases BAJA+1 y BAJA+2 en una sola, necesitamos una forma de diferenciarlas, y es aquí donde entra en juego el weight. Este parámetro nos permitirá distinguir entre ambas clases al momento de evaluarlas dentro del algoritmo."""

def calcular_ganancia(y_true, y_pred, weights):
    sorted_indices = np.argsort(y_pred)[::-1]  # Ordenar por las predicciones
    y_true = np.array(y_true)[sorted_indices]
    weights = np.array(weights)[sorted_indices]

    ganancia = np.where(y_true == 1, ganancia_acierto, -costo_estimulo)
    ganancia_acumulada = np.cumsum(ganancia * weights)
    return np.max(ganancia_acumulada)

# Crear datasets para LightGBM
lgb_train = lgb.Dataset(X_train, label=y_train, weight=w_train)

# Generar 500 semillas aleatorias fijas para Optuna
random_seeds = np.random.randint(1, 10**6, size=500)

def objective(trial):
    trial_idx = trial.number
    seed = random_seeds[trial_idx % len(random_seeds)] + trial_idx

    # Configuración de los parámetros del modelo
    params = {
        'objective': 'binary',
        'metric': 'None',
        'boosting_type': 'gbdt',
        'feature_pre_filter': False,
        'num_leaves': trial.suggest_int('num_leaves', 8, 150),
        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.25),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 5000),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.1, 1.0),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 10.0),
        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 10.0),
        'max_depth': trial.suggest_int('max_depth', 3, 50),
        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.0, 1.0),
        'verbose': -1,
        'seed': seed,  # Semilla única
    }

    # Configuración de KFold para la validación cruzada
    kf = KFold(n_splits=3, shuffle=True, random_state=seed)  # Cambiado a 3 folds

    ganancia_promedio = 0
    ganancia_test_acumulada = 0  # Variable para acumular la ganancia en el conjunto de test

    for train_index, val_index in kf.split(X_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]
        w_train_fold, w_val_fold = w_train.iloc[train_index], w_train.iloc[val_index]

        # Crear datasets de LightGBM para el fold
        lgb_train_fold = lgb.Dataset(X_train_fold, label=y_train_fold, weight=w_train_fold)
        lgb_val_fold = lgb.Dataset(X_val_fold, label=y_val_fold, weight=w_val_fold, reference=lgb_train_fold)

        # Entrenar el modelo
        model = lgb.train(
            params,
            lgb_train_fold,
            valid_sets=[lgb_train_fold, lgb_val_fold],
            num_boost_round=1000,
            callbacks=[lgb.early_stopping(stopping_rounds=100)],
            feval=lambda y_pred, data: (
                'ganancia',
                calcular_ganancia(data.get_label(), y_pred, data.get_weight()),
                True
            )
        )

        # Predecir en el conjunto de validación
        y_pred_val = model.predict(X_val_fold, num_iteration=model.best_iteration)
        ganancia_fold = calcular_ganancia(y_val_fold, y_pred_val, w_val_fold)
        ganancia_promedio += ganancia_fold

        # Predecir sobre el conjunto de test en cada iteración
        y_pred_test = model.predict(X_test, num_iteration=model.best_iteration)
        ganancia_test_iter = calcular_ganancia(y_test, y_pred_test, w_test)
        ganancia_test_acumulada += ganancia_test_iter

    # Calcular ganancia promedio de los folds
    ganancia_promedio /= 3

    # Calcular la ganancia promedio en el conjunto de prueba
    ganancia_test_promedio = ganancia_test_acumulada / 3

    # Devolver la ganancia en el conjunto de prueba como el objetivo
    return ganancia_test_promedio

# Definir la ubicación del almacenamiento y el nombre del estudio para Optuna
storage_name = "sqlite:///optimization_lgbm.db"
study_name = "exp_lgbm_13"

# Crear y ejecutar el estudio de Optuna
study = optuna.create_study(
    direction="maximize",
    study_name=study_name,
    storage=storage_name,
    load_if_exists=True
)

# Configurar Optuna
study.optimize(objective, n_trials = 30)

# Mejor resultado
print(f"Mejor ganancia: {study.best_value}")
print(f"Mejores hiperparámetros: {study.best_params}")

plot_param_importances(study)

# Parámetros óptimos obtenidos de la optimización de Optuna
best_iter = study.best_trial.user_attrs["best_iter"]
print(f"Mejor cantidad de árboles para el mejor modelo: {best_iter}")

"""Y finalmente tomamos el mejor modelo y lo entrenamos con la totalidad de los
datos
"""

# Definir los parámetros del modelo con los mejores hiperparámetros encontrados (Se tildó lo de arriba y no quedó guardado. Los pongo a mano abajo)
# params = {
#     'objective': 'binary',
#     'boosting_type': 'gbdt',
#     'first_metric_only': True,
#     'boost_from_average': True,
#     'feature_pre_filter': False,
#     'max_bin': 31,
#     'num_leaves': study.best_trial.params['num_leaves'],
#     'learning_rate': study.best_trial.params['learning_rate'],
#     'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],
#     'feature_fraction': study.best_trial.params['feature_fraction'],
#     'bagging_fraction': study.best_trial.params['bagging_fraction'],
#     'max_depth': study.best_trial.params['max_depth'],
#     'lambda_l1': study.best_trial.params['lambda_l1'],
#     'lambda_l2': study.best_trial.params['lambda_l2'],
#     'min_gain_to_split': study.best_trial.params['min_gain_to_split'],
#     'bagging_freq': study.best_trial.params['bagging_freq'],
#     'seed': 11,
#     'verbose': 0
# }

# Definir los parámetros del modelo con los mejores hiperparámetros encontrados
params = {
    'objective': 'binary',
    'boosting_type': 'gbdt',
    'first_metric_only': True,
    'boost_from_average': True,
    'feature_pre_filter': False,
    'max_bin': 31,
    'num_leaves': 64,
    'learning_rate': 0.09003784184754302,
    'min_data_in_leaf': 3202,
    'feature_fraction': 0.9931996706221722,
    'bagging_fraction': 0.12004074552445335,
    'max_depth': 35,
    'lambda_l1': 4.735850108229597,
    'lambda_l2': 9.756082154423169,
    'min_gain_to_split': 0.13794398844018246,
    'bagging_freq': 2,
    'seed': 11,
    'verbose': 0
}

# Preparación de los datos de entrenamiento (hasta abril 2021) y validación (mayo y junio 2021)
mes_train = ["202008", "202009", "202010", "202011",
             "202012", "202101", "202102", "202103", "202104"]
mes_val = ["202105", "202106"]

# Datos de entrenamiento
train_data = data[data['foto_mes'].isin(mes_train)].drop(['foto_mes', 'numero_de_cliente'], axis=1)
X_train = train_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria'], axis=1)
y_train = train_data['clase_binaria']
w_train = train_data['clase_peso']

# Datos de validación (junio 2021)
val_data = data[data['foto_mes'].isin(mes_val)].drop(['foto_mes', 'numero_de_cliente'], axis=1)
X_val = val_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria'], axis=1)
y_val = val_data['clase_binaria']
w_val = val_data['clase_peso']

# Crear datasets de LightGBM
lgb_train = lgb.Dataset(X_train, label=y_train, weight=w_train)
lgb_val = lgb.Dataset(X_val, label=y_val, weight=w_val, reference=lgb_train)

# Entrenar el modelo
model = lgb.train(params,
                  lgb_train,
                  num_boost_round=500,
                  valid_sets=[lgb_train, lgb_val],
                  callbacks=[lgb.early_stopping(stopping_rounds=10)])

importances = model.feature_importance()
feature_names = X_train.columns.tolist()
importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
importance_df = importance_df.sort_values('importance', ascending=False)
importance_df[importance_df['importance'] > 0]

# Obtener e imprimir las variables que entraron en el modelo
print("Variables utilizadas en el modelo:")
print(model.feature_name())

print(data_agosto_2021.columns.tolist())

# Selección de las variables del modelo
X_agosto_2021 = data_agosto_2021[['mpayroll', 'mcuenta_corriente', 'mrentabilidad', 'mrentabilidad_annual',
                                  'mcomisiones', 'ctrx_quarter', 'mtransferencias_emitidas',
                                  'ctarjeta_visa_transacciones', 'mtarjeta_visa_consumo', 'cproductos',
                                  'percentile_mpayroll', 'percentile_mcuenta_corriente',
                                  'percentile_mrentabilidad', 'percentile_mrentabilidad_annual',
                                  'percentile_mcomisiones', 'percentile_ctrx_quarter',
                                  'percentile_mtransferencias_emitidas', 'percentile_ctarjeta_visa_transacciones',
                                  'percentile_mtarjeta_visa_consumo', 'percentile_cproductos']]

# Realizar predicciones para el periodo 202108
y_pred_agosto_2021 = model.predict(X_agosto_2021, num_iteration=model.best_iteration)

# Calcular la ganancia utilizando las probabilidades predichas
def ganancia_prob(y_pred, y_true, prop=1):
    ganancia = np.where(y_true == 1, ganancia_acierto, 0) - np.where(y_true == 0, costo_estimulo, 0)
    return ganancia[y_pred >= 0.025].sum() / prop

# Agregar las probabilidades al DataFrame
data_agosto_2021['pred_prob'] = y_pred_agosto_2021

# Ordenar las predicciones por probabilidad en orden descendente
data_agosto_2021 = data_agosto_2021.sort_values(by='pred_prob', ascending=False)

# Marcar las primeras X predicciones como positivas
X = 6500
data_agosto_2021['Predicted'] = 0  # Inicializar todos como negativos
data_agosto_2021.iloc[:X, data_agosto_2021.columns.get_loc('Predicted')] = 1  # Marcar las primeras X como positivos

# Crear el archivo de salida con las columnas solicitadas
output = data_agosto_2021[['numero_de_cliente', 'Predicted']]

# Guardar el archivo en formato CSV para Kaggle
nombre_archivo = 'predicciones_kaggle_202108.csv'
output.to_csv(nombre_archivo, index=False)
print(f"Archivo {nombre_archivo} guardado.")

# Descargar el archivo para Kaggle
from google.colab import files
files.download(nombre_archivo)

"""Prediccion de abril"""

output.shape